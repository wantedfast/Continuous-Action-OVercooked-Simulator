
# ğŸ² Continuous OVercooked Simulator in Multi Agent Reinforcemnet Learning

This project implements a **multi-agent reinforcement learning** environment inspired by *Overcooked*.  
Two agents must cooperate implicitly (without pre-defined roles, they will chose their role by their own decesion based on environment feedback) to complete cooking tasks such as:
- Picking up onions and dishes  
- Cooking in a pot  
- Delivering finished soup to a counter  


## ğŸ“¦ Environment Setup


This project uses **Unity ML-Agents** package with **conda** envionment package management tool.

### 1. Create and activate a conda environment (About the python version, you can take reference from Unity Ml-Agents guideline. It's better to set up the environment step by step followed by Unity Ml-Agnets guide)

import COVS.unitypackage to your unity 3D project.

And then create you own environment by conda.
```bash
conda create -n overcooked
conda activate overcooked
````
---

## ğŸš€ Training

Run training with your configuration file (`run.yaml`):

```bash
mlagents-learn run.yaml --run-id=<your_run_id> --force
```

* `run.yaml` â†’ defines PPO/MAPPO training parameters
* `--run-id` â†’ unique identifier for each experiment
* `--force` â†’ overwrite previous runs with the same ID

---

## ğŸ“Š Monitoring with TensorBoard

Start TensorBoard to visualize training curves:

```bash
tensorboard --logdir results/<your_run_id>
```

You can monitor:

* **Cumulative Reward** â†’ overall performance of agents
* **Episode Length** â†’ efficiency of task completion
* **Policy & Value Losses** â†’ training stability

---

## ğŸ¤ Cooperation

* Two agents interact in the same environment
* Role allocation is **implicit**, not pre-defined
* Agents must coordinate to achieve maximum reward by finishing and delivering cooked food

---

---

## ğŸ“œ Usage Recap
```bash
conda activate overcooked
mlagents-learn run.yaml --run-id=test_run --force
tensorboard --logdir results/test_run
```

```
```
